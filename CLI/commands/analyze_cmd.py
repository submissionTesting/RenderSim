#!/usr/bin/env python3
"""
RenderSim Unified Analysis Command

Complete end-to-end pipeline from DAG pkl to comprehensive analysis results.
"""

import os
import sys
import json
import shutil
from pathlib import Path
from typing import Optional
import time

# Add project paths
project_root = Path(__file__).parent.parent.parent
sys.path.insert(0, str(project_root))
sys.path.insert(0, str(project_root / 'Instrumentation'))

from commands.map_cmd import run_map_command
from commands.schedule_cmd import run_schedule_command  
from commands.report_cmd import run_report_command

try:
	from Instrumentation.plot_dot_subgraph import main as plot_dot_subgraph_main
except Exception:
	plot_dot_subgraph_main = None

def create_analysis_summary(output_dir: Path, dag_stats: dict, mapping_stats: dict, 
                          scheduling_stats: dict) -> str:
    """Create a comprehensive analysis summary."""
    
    summary = f"""# RenderSim Analysis Summary

## Input
- **DAG File**: {dag_stats.get('source_file', 'N/A')}
- **Nodes**: {dag_stats.get('total_nodes', 0):,}
- **Edges**: {dag_stats.get('total_edges', 0):,}

## Transformation Results
- **Operators Created**: {dag_stats.get('operators_created', 0):,}
- **Total FLOPs**: {dag_stats.get('total_flops', 0):,}
- **Memory Required**: {dag_stats.get('total_memory_mb', 0):.1f} MB

## Hardware Mapping
- **Accelerator**: {mapping_stats.get('accelerator', 'N/A')}
- **Hardware Units**: {mapping_stats.get('hardware_units', 0)}
- **Mapping Distribution**:
"""
    
    for hw_type, count in mapping_stats.get('mapping_distribution', {}).items():
        summary += f"  - {hw_type}: {count} operators\n"
    
    summary += f"""
## Scheduling Results
- **Total Cycles**: {scheduling_stats.get('total_cycles', 0)}
- **Scheduling Efficiency**: {scheduling_stats.get('efficiency', 0):.3f}
- **Resource Balance**: {scheduling_stats.get('resource_balance', 0):.3f}

## Power, Performance, Area
- **Total Power**: {scheduling_stats.get('total_power_uw', 0):.1f} μW
- **Total Area**: {scheduling_stats.get('total_area_um2', 0):.1f} μm²
- **Peak Memory**: {scheduling_stats.get('peak_memory_kb', 0):.1f} KB

## Generated Files
- `mapped_ir.json` - Hardware mapping results
- `scheduled_ir.json` - Execution schedule
- `analysis_report.html` - Interactive report
- `summary.md` - This summary
- `statistics.json` - Raw data for further analysis

Generated by RenderSim Neural Rendering Accelerator Simulator
"""
    
    return summary

def extract_dag_statistics(dag_file: str) -> dict:
    """Extract statistics from the DAG transformation process."""
    
    # Import the DAG transformation module
    from dag_to_operators_integration import load_and_transform_traced_dag
    
    print(f"Analyzing DAG file: {dag_file}")
    
    try:
        scheduler_graph, impact = load_and_transform_traced_dag(dag_file)

        # Base stats from the graph
        stats = {
            'source_file': dag_file,
            'total_nodes': len(scheduler_graph.nodes),
            'total_edges': len(scheduler_graph.edges),
            'operators_created': 0,
            'total_flops': 0,
            'total_memory_mb': 0.0,
        }

        # Best-effort parse from impact (which may contain formatted strings)
        try:
            def _parse_int_commas(val):
                if isinstance(val, (int, float)):
                    return int(val)
                s = str(val).replace(',', '').strip()
                if '/' in s:
                    s = s.split('/')[0]
                for tok in ['MB', 'Gb', 'gb', 'GB']:
                    if s.endswith(tok):
                        s = s[:-len(tok)].strip()
                return int(float(s))
            def _parse_float_units(val):
                if isinstance(val, (int, float)):
                    return float(val)
                s = str(val).replace(',', '').strip().split()[0]
                return float(s)
            ts = (impact or {}).get('transformation_summary', {})
            stats['operators_created'] = _parse_int_commas(ts.get('nodes_processed', 0))
            stats['total_flops'] = _parse_int_commas(ts.get('total_flops', 0))
            stats['total_memory_mb'] = _parse_float_units(ts.get('total_memory_mb', 0.0))
        except Exception:
            pass

        # Safe prints (avoid grouped formatting on non-numeric)
        try:
            print(f"   nodes={stats['total_nodes']} edges={stats['total_edges']}")
            print(f"   flops={stats['total_flops']} mem_mb={stats['total_memory_mb']:.1f}")
        except Exception:
            pass

        return stats

    except Exception as e:
        print(f"   Error analyzing DAG: {e}")
        return {
            'source_file': dag_file,
            'total_nodes': 0,
            'total_edges': 0,
            'operators_created': 0,
            'total_flops': 0,
            'total_memory_mb': 0.0,
        }

def run_analyze_command(args, verbose: bool = False):
    """Run the unified analysis command."""
    
    # Validate inputs
    dag_file = Path(args.dag_file)
    if not dag_file.exists():
        raise FileNotFoundError(f"DAG file not found: {dag_file}")
    
    output_dir = Path(args.output_dir)
    output_dir.mkdir(parents=True, exist_ok=True)
    
    # Create structured output subdirectories
    inputs_dir = output_dir / 'inputs'
    mapping_dir = output_dir / 'mapping'
    scheduling_dir = output_dir / 'scheduling'
    reports_dir = output_dir / 'reports'
    visuals_dir = output_dir / 'visuals'
    for d in (inputs_dir, mapping_dir, scheduling_dir, reports_dir, visuals_dir):
        d.mkdir(parents=True, exist_ok=True)
    
    # Copy input DAG and optional hints into inputs/
    try:
        shutil.copy2(dag_file, inputs_dir / 'execution_dag.pkl')
    except Exception:
        pass
    try:
        hints_path = Path('optimization_hints.json')
        if hints_path.exists():
            shutil.copy2(hints_path, inputs_dir / 'optimization_hints.json')
    except Exception:
        pass

    # Default hardware config if not specified
    if hasattr(args, 'hardware') and args.hardware:
        hardware_config = args.hardware
        # If it's just a filename, prepend the examples path
        if not os.path.sep in hardware_config:
            hardware_config = str(project_root / 'Hardware' / 'examples' / 'hardware_configs' / hardware_config)
    else:
        # Use ICARUS as default
        hardware_config = str(project_root / 'Hardware' / 'examples' / 'hardware_configs' / 'icarus_config.json')
        if verbose:
            print(f"Using default hardware config: {hardware_config}")
    
    if not Path(hardware_config).exists():
        raise FileNotFoundError(f"Hardware config not found: {hardware_config}")
    
    print("RenderSim Unified Analysis Pipeline")
    print(f"   Input DAG: {dag_file}")
    print(f"   Output Directory: {output_dir}")
    print(f"   Hardware Config: {Path(hardware_config).name}")
    print("=" * 60)
    
    # Step 1: Extract DAG statistics
    t0 = time.time()
    dag_stats = extract_dag_statistics(str(dag_file))
    t1 = time.time()
    if verbose:
        print(f"   DAG analysis time: {t1 - t0:.2f}s")
    
    # Step 2: Map operators to hardware
    print("\nStep 1: Mapping operators to hardware...")
    mapped_file = mapping_dir / 'mapped_ir.json'
    
    # Create a mock args object for map command
    class MapArgs:
        def __init__(self):
            self.execution_dag = str(dag_file)
            self.hardware_config = hardware_config
            self.output = str(mapped_file)
    
    map_args = MapArgs()
    # Propagate flags
    setattr(map_args, 'basic_parser', getattr(args, 'basic_parser', False))
    t2 = time.time()
    run_map_command(map_args, verbose)
    t3 = time.time()
    if verbose:
        print(f"   Mapping time: {t3 - t2:.2f}s")
    
    # Extract mapping statistics
    with open(mapped_file, 'r') as f:
        mapped_data = json.load(f)
    
    mapping_stats = {
        'accelerator': mapped_data.get('accelerator') or mapped_data.get('accelerator_name', 'Unknown'),
        'hardware_units': mapped_data.get('hardware_unit_count', 0) or len(mapped_data.get('hardware_units', {})),
        'mapping_distribution': {}
    }
    # Prefer new structure: mapped_ir.nodes
    try:
        nodes = mapped_data.get('mapped_ir', {}).get('nodes', {})
        if isinstance(nodes, dict) and nodes:
            for _nid, ninfo in nodes.items():
                hw_unit = (ninfo or {}).get('hw_unit', 'Unknown')
                mapping_stats['mapping_distribution'][hw_unit] = mapping_stats['mapping_distribution'].get(hw_unit, 0) + 1
        else:
            # Fallback to legacy list structure if present
            for op in mapped_data.get('operators', []):
                hw_type = op.get('hardware_type', 'Unknown')
                mapping_stats['mapping_distribution'][hw_type] = mapping_stats['mapping_distribution'].get(hw_type, 0) + 1
    except Exception:
        pass
    
    # Step 3: Schedule execution
    print("\nStep 2: Scheduling execution...")
    scheduled_file = scheduling_dir / 'scheduled_ir.json'
    
    class ScheduleArgs:
        def __init__(self):
            self.mapped_ir = str(mapped_file)
            self.output = str(scheduled_file)
            self.hardware = hardware_config
    
    schedule_args = ScheduleArgs()
    setattr(schedule_args, 'no_ppa', getattr(args, 'no_ppa', False))
    setattr(schedule_args, 'reuse_op_cache', getattr(args, 'reuse_op_cache', False))
    t4 = time.time()
    run_schedule_command(schedule_args, verbose)
    t5 = time.time()
    if verbose:
        print(f"   Scheduling time: {t5 - t4:.2f}s")
    
    # Extract scheduling statistics
    with open(scheduled_file, 'r') as f:
        scheduled_data = json.load(f)
    
    scheduling_stats = {
        'total_cycles': scheduled_data.get('system_schedule', {}).get('total_cycles', scheduled_data.get('total_cycles', 0)),
        'efficiency': scheduled_data.get('system_statistics', {}).get('scheduling_efficiency', scheduled_data.get('efficiency', 0.0)),
        'resource_balance': scheduled_data.get('system_statistics', {}).get('resource_balance_factor', scheduled_data.get('resource_balance', 0.0)),
        'total_power_uw': scheduled_data.get('ppa_metrics', {}).get('total_power_uw', scheduled_data.get('total_power_uw', 0.0)),
        'total_area_um2': scheduled_data.get('ppa_metrics', {}).get('total_area_um2', scheduled_data.get('total_area_um2', 0.0)),
        'peak_memory_kb': scheduled_data.get('ppa_metrics', {}).get('peak_memory_kb', scheduled_data.get('peak_memory_kb', 0.0))
    }
    
    # Step 4: Generate comprehensive report
    print("\nStep 3: Generating report...")
    # Choose report filename by format
    fmt = getattr(args, 'report_format', 'html') or 'html'
    ext = 'html' if fmt == 'html' else ('json' if fmt == 'json' else 'txt')
    report_file = reports_dir / f"analysis_report.{ext}"
    
    class ReportArgs:
        def __init__(self):
            self.schedule = str(scheduled_file)
            self.output = str(report_file)
            self.format = fmt
    
    report_args = ReportArgs()
    t6 = time.time()
    run_report_command(report_args, verbose)
    t7 = time.time()
    if verbose:
        print(f"   Report time: {t7 - t6:.2f}s")
    
    # Step 5: Create summary and statistics
    print("\nStep 4: Creating analysis summary...")
    
    # Create markdown summary
    summary_content = create_analysis_summary(output_dir, dag_stats, mapping_stats, scheduling_stats)
    summary_file = reports_dir / 'summary.md'
    with open(summary_file, 'w') as f:
        f.write(summary_content)
    
    # Create JSON statistics for programmatic access
    stats_data = {
        'dag_statistics': dag_stats,
        'mapping_statistics': mapping_stats,
        'scheduling_statistics': scheduling_stats,
        'files_generated': [
            str(mapped_file.relative_to(output_dir)),
            str(scheduled_file.relative_to(output_dir)),
            str(report_file.relative_to(output_dir)),
            str((reports_dir / 'summary.md').relative_to(output_dir)),
            str((reports_dir / 'statistics.json').relative_to(output_dir))
        ]
    }
    
    stats_file = reports_dir / 'statistics.json'
    with open(stats_file, 'w') as f:
        json.dump(stats_data, f, indent=2)
    
    # Step 6: Copy visualization files (optional)
    if not getattr(args, 'no_visuals', False):
        print("\nStep 5: Copying visualization files...")
        visualization_files_copied = 0
        # Copy DAG visualization files
        for png_file in Path('.').glob('*.png'):
            if any(keyword in png_file.name.lower() for keyword in ['dag', 'execution', 'pipeline', 'operator', 'neural_rendering']):
                dest_file = visuals_dir / png_file.name
                shutil.copy2(png_file, dest_file)
                visualization_files_copied += 1
                if verbose:
                    print(f"   Copied DAG/Pipeline: {png_file.name}")
        # Copy operator visualization files from Operators directory
        operators_dir = project_root / 'Operators'
        if operators_dir.exists():
            for png_file in operators_dir.glob('*.png'):
                dest_file = visuals_dir / f"operators_{png_file.name}"
                shutil.copy2(png_file, dest_file)
                visualization_files_copied += 1
                if verbose:
                    print(f"   Copied Operator: {png_file.name}")
        # Copy roofline and analysis plots if they exist
        for png_file in Path('.').glob('*roofline*.png'):
            dest_file = visuals_dir / png_file.name
            shutil.copy2(png_file, dest_file)
            visualization_files_copied += 1
            if verbose:
                print(f"   Copied Roofline: {png_file.name}")
        print(f"   Copied {visualization_files_copied} visualization files")
    else:
        if verbose:
            print("   Skipping visualization copying (--no-visuals)")
    
    # Optional: extract and render a DOT subgraph cluster into visuals/
    if getattr(args, 'plot_dot_subgraph', False) and plot_dot_subgraph_main and getattr(args, 'dot_path', None) and getattr(args, 'cluster_index', None):
        print("\nSubgraph: extracting and rendering requested DOT cluster...")
        # Build out-prefix inside visuals/
        out_prefix = args.subgraph_out_prefix or 'execution_dag_component'
        out_prefix_path = (visuals_dir / out_prefix)
        # Execute helper by temporarily adjusting argv
        import sys as _sys
        old_argv = list(_sys.argv)
        _sys.argv = [
            "plot_dot_subgraph.py",
            "--dot", str(args.dot_path),
            "--cluster-index", str(args.cluster_index),
            "--out-prefix", str(out_prefix_path)
        ]
        try:
            plot_dot_subgraph_main()
        finally:
            _sys.argv = old_argv
    
    # Final summary
    print("\nAnalysis Complete.")
    print(f"   All results saved to: {output_dir}")
    print(f"   Summary: {summary_file}")
    print(f"   Report: {report_file}")
    print(f"   Raw Statistics: {stats_file}")
    
    generated_files = list(output_dir.glob('*'))
    print(f"   Total files generated: {len(generated_files)}")
    
    return 0 