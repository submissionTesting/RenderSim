#!/usr/bin/env python3
"""
Semantic DAG Parser for NeRF Pipelines

This module loads semantic execution DAGs generated by the new semantic tracing approach
and converts them to RenderSim's OperatorGraph format with proper 4-stage taxonomy mapping.
"""

import pickle
import json
from typing import Dict, Any, List, Tuple
import networkx as nx
from pathlib import Path

try:
    from .IR import OperatorGraph, OperatorNode, TensorDesc
except ImportError:
    from IR import OperatorGraph, OperatorNode, TensorDesc

try:
    from ..Instrumentation.operator_mapping import map_operator_to_hardware_type
except ImportError:
    import sys
    sys.path.append('Instrumentation')
    from operator_mapping import map_operator_to_hardware_type


def load_semantic_execution_dag(dag_path: str) -> OperatorGraph:
    """
    Load a semantic execution DAG and convert to RenderSim OperatorGraph.
    
    This function handles DAGs generated by the semantic NeRF tracing approach,
    which focuses on the 4-stage neural rendering taxonomy.
    
    Args:
        dag_path: Path to the semantic execution DAG pickle file
        
    Returns:
        OperatorGraph: RenderSim operator graph with semantic 4-stage taxonomy
    """
    print(f"Loading Semantic NeRF DAG: {dag_path}")
    
    # Load the semantic DAG data
    try:
        with open(dag_path, 'rb') as f:
            dag_data = pickle.load(f)
    except Exception as e:
        print(f"Error loading DAG: {e}")
        raise
    
    # Validate semantic DAG format
    if not isinstance(dag_data, dict) or 'metadata' not in dag_data:
        raise ValueError(f"Invalid semantic DAG format in {dag_path}")
    
    metadata = dag_data['metadata']
    nodes_data = dag_data.get('nodes', {})
    edges_data = dag_data.get('edges', [])
    
    print(f"Semantic DAG Info:")
    print(f"   Pipeline type: {metadata.get('pipeline_type', 'Unknown')}")
    print(f"   Total nodes: {metadata.get('total_nodes', len(nodes_data))}")
    print(f"   Semantic stages: {metadata.get('semantic_stages', {})}")
    
    # Extract pipeline dimensions
    dimensions = metadata.get('dimensions', {})
    rays = dimensions.get('rays', 4096)
    samples_per_ray = dimensions.get('samples_per_ray', 64)
    encoding_dim = dimensions.get('encoding_dim', None) or 60  # Default to 60 if None
    
    print(f"   Rays: {rays}, Samples/ray: {samples_per_ray}")
    print(f"   Encoding dimension: {encoding_dim}")
    
    # Create OperatorGraph
    operator_graph = OperatorGraph()
    
    # Process semantic nodes
    for node_id, node_info in nodes_data.items():
        semantic_stage = node_info.get('semantic_stage', 'UNKNOWN')
        function_name = node_info.get('function_name', node_id)
        call_count = node_info.get('call_count', 1)
        execution_time = node_info.get('execution_time', 0.0)
        
        # Map semantic stage to hardware type  
        hardware_type = map_operator_to_hardware_type(semantic_stage)
        
        # Estimate realistic tensor dimensions based on semantic stage
        input_dims, output_dims = estimate_tensor_dimensions(
            semantic_stage, rays, samples_per_ray, encoding_dim
        )
        
        # Create tensor descriptors
        inputs = [TensorDesc(shape=input_dims, dtype='float32')]
        outputs = [TensorDesc(shape=output_dims, dtype='float32')]
        
        # Create operator node with semantic information
        operator_node = OperatorNode(
            id=node_id,
            op_type=semantic_stage,  # Use semantic stage as op_type
            inputs=inputs,
            outputs=outputs,
            call_count=call_count,
            metadata={
                'function_name': function_name,
                'semantic_stage': semantic_stage,
                'hardware_type': hardware_type,
                'execution_time': execution_time,
                'input_elements': _calculate_elements(input_dims),
                'output_elements': _calculate_elements(output_dims),
                'semantic_tracing': True,
                'rays': rays,
                'samples_per_ray': samples_per_ray
            }
        )
        
        operator_graph.nodes[node_id] = operator_node
    
    # Process edges
    for edge in edges_data:
        src = edge['source']
        dst = edge['target']
        if src in operator_graph.nodes and dst in operator_graph.nodes:
            operator_graph.edges.append((src, dst))
    
    print(f"Converted to OperatorGraph: {len(operator_graph.nodes)} nodes, {len(operator_graph.edges)} edges")
    
    # Print semantic stage distribution
    stage_counts = {}
    for node in operator_graph.nodes.values():
        stage = node.op_type
        stage_counts[stage] = stage_counts.get(stage, 0) + 1
    
    print(f"Semantic Stage Distribution:")
    for stage, count in sorted(stage_counts.items()):
        hw_type = map_operator_to_hardware_type(stage)
        print(f"   {stage}: {count} ops -> {hw_type}")
    
    return operator_graph


def estimate_tensor_dimensions(semantic_stage: str, rays: int, samples_per_ray: int, 
                             encoding_dim: int) -> Tuple[List[int], List[int]]:
    """
    Estimate realistic tensor dimensions based on semantic stage and pipeline parameters.
    
    Args:
        semantic_stage: The semantic stage (SAMPLING, ENCODING, FIELD_COMPUTATION, BLENDING)
        rays: Number of rays
        samples_per_ray: Number of samples per ray
        encoding_dim: Encoding output dimension
        
    Returns:
        Tuple of (input_dimensions, output_dimensions)
    """
    total_samples = rays * samples_per_ray
    
    if semantic_stage == 'SAMPLING':
        # Ray sampling: Input camera rays, output ray samples
        input_dims = [rays, 6]  # Ray origins + directions
        output_dims = [total_samples, 3]  # Sample positions
        
    elif semantic_stage == 'ENCODING':
        # Encoding: Input 3D positions, output encoded features
        input_dims = [total_samples, 3]  # 3D positions
        output_dims = [total_samples, encoding_dim]  # Encoded features
        
    elif semantic_stage == 'FIELD_COMPUTATION':
        # MLP field computation: Input encoded features, output density + color
        input_dims = [total_samples, encoding_dim]  # Encoded features
        output_dims = [total_samples, 4]  # Density (1) + RGB (3)
        
    elif semantic_stage == 'BLENDING':
        # Volume rendering: Input density + color, output final pixel colors
        input_dims = [total_samples, 4]  # Density + RGB
        output_dims = [rays, 3]  # Final RGB per ray
        
    else:
        # Default fallback
        input_dims = [total_samples, 1]
        output_dims = [total_samples, 1]
    
    return input_dims, output_dims


def _calculate_elements(dims: List[int]) -> int:
    """Calculate total number of elements in tensor dimensions."""
    result = 1
    for dim in dims:
        if dim is not None:
            result *= dim
        else:
            result *= 1  # Treat None as dimension of 1
    return result


def convert_legacy_to_semantic(legacy_dag_path: str, semantic_dag_path: str = None) -> str:
    """
    Convert a legacy (granular) DAG to semantic DAG format.
    
    This is a helper function for cases where we have the old 72-node granular DAGs
    and want to convert them to the semantic 4-stage format.
    
    Args:
        legacy_dag_path: Path to legacy execution DAG
        semantic_dag_path: Output path for semantic DAG (auto-generated if None)
        
    Returns:
        Path to the converted semantic DAG
    """
    if semantic_dag_path is None:
        legacy_path = Path(legacy_dag_path)
        semantic_dag_path = str(legacy_path.with_name(f"semantic_{legacy_path.name}"))
    
    print("Converting Legacy DAG to Semantic Format...")
    print(f"   Input: {legacy_dag_path}")
    print(f"   Output: {semantic_dag_path}")
    
    # Load legacy DAG
    with open(legacy_dag_path, 'rb') as f:
        legacy_data = pickle.load(f)
    
    # Handle both NetworkX DiGraph and dictionary formats
    if hasattr(legacy_data, 'nodes') and hasattr(legacy_data, 'edges'):
        # NetworkX DiGraph format
        legacy_nodes = dict(legacy_data.nodes(data=True))
        legacy_edges = [{'source': src, 'target': dst} for src, dst in legacy_data.edges()]
    else:
        # Dictionary format
        legacy_nodes = legacy_data.get('nodes', {})
        legacy_edges = legacy_data.get('edges', [])
    
    # Simple conversion logic - group nodes by semantic stage
    semantic_nodes = {}
    semantic_edges = []
    stage_counters = {'SAMPLING': 0, 'ENCODING': 0, 'FIELD_COMPUTATION': 0, 'BLENDING': 0}
    
    # Map legacy nodes to semantic stages
    node_mapping = {}  # legacy_id -> semantic_id
    
    for node_id, node_data in legacy_nodes.items():
        function_name = node_data.get('function_name', str(node_id))
        
        # Determine semantic stage based on function name
        semantic_stage = _map_function_to_semantic_stage(function_name)
        
        # Create semantic node ID
        stage_id = stage_counters[semantic_stage]
        stage_counters[semantic_stage] += 1
        semantic_node_id = f"{semantic_stage}_{stage_id}_{function_name.split('.')[-1]}"
        
        node_mapping[node_id] = semantic_node_id
        
        # Create semantic node
        semantic_nodes[semantic_node_id] = {
            'function_name': function_name,
            'semantic_stage': semantic_stage,
            'call_count': 1,
            'execution_time': 0.0,
            'timestamp': 0.0
        }
    
    # Map edges to semantic DAG
    processed_edges = set()
    for edge in legacy_edges:
        src = edge.get('source')
        dst = edge.get('target')
        
        if src in node_mapping and dst in node_mapping:
            semantic_src = node_mapping[src]
            semantic_dst = node_mapping[dst]
            
            # Avoid duplicate edges
            edge_key = (semantic_src, semantic_dst)
            if edge_key not in processed_edges and semantic_src != semantic_dst:
                semantic_edges.append({'source': semantic_src, 'target': semantic_dst})
                processed_edges.add(edge_key)
    
    # Create semantic DAG data
    semantic_data = {
        'metadata': {
            'pipeline_type': 'NeRF',
            'total_nodes': len(semantic_nodes),
            'total_edges': len(semantic_edges),
            'dimensions': {
                'rays': 4096,
                'samples_per_ray': 64,
                'total_samples': 262144,
                'encoding_dim': 60
            },
            'semantic_stages': dict(stage_counters),
            'converted_from_legacy': True,
            'timestamp': 0.0
        },
        'nodes': semantic_nodes,
        'edges': semantic_edges
    }
    
    # Save semantic DAG
    with open(semantic_dag_path, 'wb') as f:
        pickle.dump(semantic_data, f)
    
    print(f"Converted {len(legacy_nodes)} legacy nodes to {len(semantic_nodes)} semantic nodes")
    print(f"Semantic stages: {dict(stage_counters)}")
    
    return semantic_dag_path


def _map_function_to_semantic_stage(function_name: str) -> str:
    """Map a function name to semantic stage for legacy conversion."""
    func_lower = function_name.lower()
    
    if any(term in func_lower for term in ['sampl', 'ray', 'frustum', 'weight']):
        return 'SAMPLING'
    elif any(term in func_lower for term in ['encod', 'hash', 'rff', 'position', 'fourier']):
        return 'ENCODING'
    elif any(term in func_lower for term in ['mlp', 'network', 'field', 'density', 'rgb', 'color']):
        return 'FIELD_COMPUTATION'
    elif any(term in func_lower for term in ['render', 'blend', 'volume', 'composite']):
        return 'BLENDING'
    else:
        return 'FIELD_COMPUTATION'  # Default fallback


if __name__ == "__main__":
    # Test the semantic DAG loading
    import sys
    
    if len(sys.argv) > 1:
        dag_path = sys.argv[1]
        try:
            operator_graph = load_semantic_execution_dag(dag_path)
            print(f"Successfully loaded semantic DAG with {len(operator_graph.nodes)} operators")
        except Exception as e:
            print(f"Error: {e}")
    else:
        print("Usage: python parse_dag_semantic.py <semantic_dag_path>")
        print("   or: python parse_dag_semantic.py --convert <legacy_dag_path>")
        
        if len(sys.argv) > 2 and sys.argv[1] == "--convert":
            legacy_path = sys.argv[2]
            semantic_path = convert_legacy_to_semantic(legacy_path)
            print(f"Converted to semantic format: {semantic_path}") 